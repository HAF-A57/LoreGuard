# LoreGuard Research Summary and Implementation Recommendations

## Executive Summary

Based on comprehensive research across 18 key technology areas, we have identified the optimal technology stack and architectural decisions for LoreGuard's facts and perspectives harvesting mission. This document consolidates findings and provides specific recommendations for updating the existing planning documents.

## Technology Stack Decisions

Based on comprehensive research across **16 key technology areas**, we have identified the optimal technology stack and architectural decisions for LoreGuard. All selections prioritize on-premises deployment, DoD compliance, and enterprise scalability.

### âœ… RESOLVED: Core Infrastructure

| Component | Recommendation | Rationale |
|-----------|---------------|-----------|
| **Crawler Framework** | Scrapy + Playwright hybrid | Best balance of performance, politeness, and anti-bot evasion |
| **Document Processing** | unstructured.io + Tesseract fallback | Open-source, comprehensive format support (NO cloud dependencies) |
| **Vector Database** | pgvector (Phase 1) â†’ Weaviate (Scale) | Start simple, scale to specialized vector ops |
| **Workflow Orchestration** | Temporal (complex) + Celery (simple) | Durability for critical workflows, efficiency for simple tasks |
| **LLM Structured Output** | Function calling + JSON schema validation | Most reliable approach for evaluation pipeline |
| **Object Storage** | MinIO (on-premises S3-compatible) | Full on-premises deployment, no cloud dependencies |
| **Signal Distribution** | SharePoint + Nextcloud backup | DoD-compatible sharing with scalability research needed |
| **UI Components** | shadcn/ui + TanStack Table | Modern, performant, accessible components with virtualization |
| **Data Virtualization** | TanStack Virtual + React Table | Handle 100K+ rows with 60fps scrolling performance |
| **JSON Validation** | Pydantic v2 + Zod (frontend) | Type-safe validation with excellent error handling |
| **Evidence Storage** | WARC format + structured extraction | Industry standard for web archiving with legal compliance |

### ðŸ”„ PENDING: Specialized Areas

| Component | Status | Next Research Priority |
|-----------|--------|----------------------|
| **Multilingual Processing** | Research needed | Translation strategies for global documents |
| **Calibration Methodologies** | Research needed | Human gold-set tooling and drift detection |
| **Enterprise File Sharing** | Research needed | Nextcloud/ownCloud for Signal distribution |

## Architectural Recommendations

### Phase 1: Foundation (0-10M documents)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Crawlers  â”‚    â”‚   Document      â”‚    â”‚   PostgreSQL    â”‚
â”‚   (Scrapy)      â”‚â”€â”€â”€â–¶â”‚   Processing    â”‚â”€â”€â”€â–¶â”‚   + pgvector    â”‚
â”‚                 â”‚    â”‚   (unstructured)â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Temporal      â”‚    â”‚   LLM           â”‚    â”‚   MAGE          â”‚
â”‚   Workflows     â”‚    â”‚   Evaluation    â”‚    â”‚   Integration   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 2: Scaling (10M-100M documents)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Distributed   â”‚    â”‚   Tiered        â”‚    â”‚   Hybrid Vector â”‚
â”‚   Crawling      â”‚â”€â”€â”€â–¶â”‚   Processing    â”‚â”€â”€â”€â–¶â”‚   Storage       â”‚
â”‚   (Scrapy-Redis)â”‚    â”‚   Pipeline      â”‚    â”‚   (PG+Weaviate) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Implementation Roadmap

### Sprint 1-2: Core Foundation
1. **Set up Temporal cluster** for workflow orchestration
2. **Implement basic Scrapy crawler** with politeness controls
3. **Integrate unstructured.io** for document processing
4. **Set up pgvector** for initial semantic search

### Sprint 3-4: Evaluation Pipeline
1. **Implement LLM evaluation service** with function calling
2. **Create configurable rubric system** with JSON schema validation
3. **Build calibration framework** for quality assurance
4. **Add monitoring and alerting**

### Sprint 5-6: UI and Integration
1. **Build React frontend** with shadcn/ui three-pane layout
2. **Implement TanStack virtualization** for 100K+ row tables
3. **Create MAGE integration** via shared APIs
4. **Add SharePoint export** with WARC evidence archival

## Specific Document Updates Required

### ValueNarrative.md Updates
```markdown
### Technology Advantages (NEW SECTION)
- **Proven Open-Source Stack**: Built on battle-tested technologies (Scrapy, PostgreSQL, Temporal)
- **Scalable Architecture**: Designed to grow from thousands to millions of documents
- **Enterprise Security**: RBAC, encryption, audit trails built-in
- **Cost-Effective**: Open-source core with optional premium features

### Risk Mitigation Updates
- **Technology Risk**: Minimal - using mature, well-supported libraries
- **Vendor Lock-in**: Avoided through open-source first approach
- **Scaling Challenges**: Phased architecture handles growth smoothly
```

### TechArchitecture.md Updates
```markdown
### Service Details (EXPANDED)
- `svc-ingestion`: Scrapy-based with scrapy-playwright for JS sites
- `svc-normalize`: unstructured.io primary, GROBID for academic, Tesseract fallback
- `svc-evaluate`: OpenAI function calling with Pydantic v2 validation
- `svc-orchestrator`: Temporal workflows with Celery for simple tasks
- `svc-clarification`: Evidence collection with WARC archival
- `svc-frontend`: React + shadcn/ui with TanStack virtualization

### Storage Architecture (ON-PREMISES ONLY)
- **Object Storage**: MinIO cluster for S3-compatible storage (fully on-premises)
- **Metadata**: PostgreSQL with partitioning and GIN indexes
- **Vectors**: pgvector with HNSW indexes, Weaviate for scale
- **Cache**: Redis for rate limiting and session state
- **Signal Distribution**: SharePoint (primary) + Nextcloud (backup) for DoD sharing

### Deployment Strategy (ON-PREMISES)
- **Development**: Docker Compose with MinIO and all services
- **Staging**: Kubernetes with auto-scaling (on-premises cluster)
- **Production**: Air-gapped deployment with local failover
- **Classified**: SIPR-compatible deployment research needed
```

### EvaluationPipelinePlan.md Updates
```markdown
### LLM Integration Strategy (DETAILED)
```python
# Function calling for structured output
@openai.function_call
def evaluate_document(content: str, metadata: dict) -> EvaluationResult:
    """
    Evaluate document using configurable rubric
    
    Returns:
        EvaluationResult with scores, confidence, and reasoning
    """
    pass

# Pydantic models for validation
class EvaluationResult(BaseModel):
    scores: Dict[str, float] = Field(..., description="Category scores 0-5")
    label: Literal["Signal", "Noise", "Review"] = Field(...)
    confidence: float = Field(ge=0, le=1)
    reasoning: str = Field(..., min_length=50)
```

### Calibration Framework (NEW SECTION)
- **Gold Set Management**: Human-labeled examples across domains
- **Drift Detection**: Statistical tests for score distribution changes
- **A/B Testing**: Compare rubric versions and model configurations
- **Quality Metrics**: Precision, recall, inter-rater agreement tracking
```

### StoragePlan.md Updates
```markdown
### Vector Storage Strategy (DETAILED)
```sql
-- Phase 1: pgvector setup
CREATE TABLE artifact_vectors (
    artifact_id UUID PRIMARY KEY,
    content_vector vector(1536),
    chunk_vectors vector(1536)[],
    metadata JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX artifact_vectors_hnsw_idx 
ON artifact_vectors USING hnsw (content_vector vector_cosine_ops)
WITH (m = 16, ef_construction = 64);
```

### Hybrid Search Implementation (NEW)
- **BM25 + Vector**: Combine text and semantic similarity
- **Faceted Search**: Filter by source, time, topic, confidence
- **Chunking Strategy**: Overlapping 1000-token chunks for better retrieval
```

### NotionalRubricToStart.md Updates
```yaml
# Enhanced rubric with implementation details
version: "v1.0"
model_config:
  provider: "openai"
  model: "gpt-4"
  temperature: 0.1
  max_tokens: 1000

prompt_template: |
  You are evaluating a military document for wargaming relevance.
  
  Document: {content}
  Metadata: {metadata}
  Clarification: {clarification_signals}
  
  Evaluate using the following criteria...

validation_schema:
  type: object
  required: [scores, label, confidence, reasoning]
  properties:
    scores:
      type: object
      properties:
        credibility: {type: number, minimum: 0, maximum: 5}
        relevance: {type: number, minimum: 0, maximum: 5}
        # ... other categories
```

## Risk Assessment and Mitigation

### Technical Risks
| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Temporal learning curve | Medium | Low | Extensive documentation, training |
| pgvector performance at scale | Low | Medium | Weaviate migration path planned |
| LLM evaluation consistency | Medium | High | Calibration framework, human oversight |

### Operational Risks
| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Source blocking crawlers | High | Medium | Rotating proxies, politeness controls |
| Document format changes | Medium | Low | Multiple processing engines |
| Evaluation drift | Medium | High | Continuous monitoring, retraining |

## Cost Projections

### Development Phase (6 months)
- **Personnel**: $500K (4 engineers Ã— 6 months)
- **Infrastructure**: $15K (development environments)
- **Tools/Licenses**: $10K (monitoring, testing tools)
- **Total**: $525K

### Operational Phase (annual)
- **Infrastructure**: $50K (AWS/cloud hosting)
- **LLM API costs**: $25K (evaluation calls)
- **Monitoring/Tools**: $15K (observability stack)
- **Maintenance**: $200K (2 engineers)
- **Total**: $290K annually

## Success Metrics

### Technical KPIs
- **Throughput**: 1000+ documents processed/hour
- **Accuracy**: 90%+ precision on Signal classification
- **Availability**: 99.5% uptime
- **Latency**: <2 hours source-to-library for new documents

### Business KPIs
- **Analyst Productivity**: 5x increase in documents reviewed
- **Time-to-Insight**: 80% reduction in research time
- **Coverage**: 10x increase in source monitoring
- **Quality**: 95% analyst satisfaction with recommendations

## Next Steps

### Immediate (Week 1-2)
1. **Complete remaining research** (multilingual processing, calibration methodologies)
2. **Set up development environment** (Temporal, PostgreSQL, Redis, MinIO)
3. **Create detailed project plan** with sprints and milestones
4. **Assemble development team** and assign initial tasks

### Short-term (Month 1)
1. **Implement core crawling** with sample sources
2. **Build document processing pipeline** with unstructured.io
3. **Create basic evaluation service** with OpenAI integration
4. **Set up monitoring and logging** infrastructure

### Medium-term (Month 2-3)
1. **Build React frontend** with shadcn/ui and TanStack virtualization
2. **Implement rubric management** with Pydantic validation
3. **Add WARC evidence archival** and clarification pipeline
4. **Performance testing** and optimization for 100K+ artifacts

## Conclusion

The research phase has provided clear technology choices and architectural direction for LoreGuard. The recommended stack balances:

- **Reliability**: Proven technologies with strong community support
- **Scalability**: Architecture that grows with requirements
- **Maintainability**: Open-source tools with good documentation
- **Performance**: Optimized for the specific use case of global perspectives and insights harvesting

The phased approach allows for iterative development while maintaining a clear path to enterprise scale. With these recommendations, the development team has a solid foundation to begin implementation.
